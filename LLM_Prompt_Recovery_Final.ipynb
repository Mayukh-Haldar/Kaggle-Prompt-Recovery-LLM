{"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":67121,"databundleVersionId":7806901,"sourceType":"competition"},{"sourceId":7731345,"sourceType":"datasetVersion","datasetId":4517764},{"sourceId":7731789,"sourceType":"datasetVersion","datasetId":4509496},{"sourceId":7733314,"sourceType":"datasetVersion","datasetId":4518936},{"sourceId":7747717,"sourceType":"datasetVersion","datasetId":4506214},{"sourceId":7994533,"sourceType":"datasetVersion","datasetId":4552503},{"sourceId":11372,"sourceType":"modelInstanceVersion","modelInstanceId":5388}],"dockerImageVersionId":30683,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# LLM Prompt Recovery with Gemma\n\n<div align=\"center\">\n    <img src=\"https://i.ibb.co/8xZNc32/Gemma.png\">\n</div>\n\n**The challenge:** Recover the LLM prompt used to rewrite a given text.\n\n**KerasNLP Models:** [KerasNLP website](https://keras.io/api/keras_nlp/models/)\n\n**About Gemma Models:** Gemma is a collection of advanced open LLMs developed by `Google DeepMind` and other `Google teams`, derived from the same research and technology behind the `Gemini models`.\n\n| Parameters size | Tuned versions    | Intended platforms                 | Preset                 |\n|-----------------|-------------------|------------------------------------|------------------------|\n| 2B              | Pretrained        | Mobile devices and laptops         | `gemma_2b_en`          |\n| 2B              | Instruction tuned | Mobile devices and laptops         | `gemma_instruct_2b_en` |\n| 7B              | Pretrained        | Desktop computers and small servers| `gemma_7b_en`          |\n| 7B              | Instruction tuned | Desktop computers and small servers| `gemma_instruct_7b_en` |\n\n**Datasets:**\n1) [Rewritten texts with Gemma 2B](https://www.kaggle.com/datasets/juanmerinobermejo/rewritten-texts-with-gemma-2b) | Credits: [Juan Merino](https://www.kaggle.com/juanmerinobermejo)\n\n2) [gemma-rewrite-nbroad](https://www.kaggle.com/datasets/nbroad/gemma-rewrite-nbroad) | Credits: [Nicholas Broad](https://www.kaggle.com/nbroad)\n\n3) [LLM Prompt Recovery - Synthetic Datastore](https://www.kaggle.com/datasets/dschettler8845/llm-prompt-recovery-synthetic-datastore) | Credits: [Darien Schettler](https://www.kaggle.com/dschettler8845)\n\n4) [llm-prompt-recovery-data](https://www.kaggle.com/datasets/thedrcat/llm-prompt-recovery-data) | Credits: [Darek Kłeczek](https://www.kaggle.com/thedrcat)\n\n5) [3000 Rewritten texts - Prompt recovery Challenge](https://www.kaggle.com/datasets/dipamc77/3000-rewritten-texts-prompt-recovery-challenge) | Credits: [Dipam Chakraborty](https://www.kaggle.com/dipamc77)\n\n\n**Evaluation:** For each row in the submission and corresponding ground truth, [sentence-t5-base](https://www.kaggle.com/models/google/sentence-t5/frameworks/tensorFlow2/variations/st5-base) is used to calculate corresponding embedding vectors. The score for each predicted / expected pair is calculated using the [Sharpened Cosine Similarity](https://github.com/brohrer/sharpened-cosine-similarity), using an exponent of `3`. The SCS is used to attenuate the generous score given by embedding vectors for incorrect answers. Do not leave any `rewrite_prompt` blank as null answers will throw an error.\n\n**Requirements:**\n* CPU Notebook <= 9 hours run-time\n* GPU Notebook <= 9 hours run-time\n* Internet access disabled\n* Freely & publicly available external data is allowed, including pre-trained models\n* Submission file must be named submission.csv\n* Submission runtimes have been slightly obfuscated. If you repeat the exact same submission you  will see up to 15 minutes of variance in the time before you receive your score.\n\n\n*THIS NOTEBOOK IS BASED ON THE \"Prompt Recovery with Gemma - KerasNLP Starter\" NOTEBOOK! [LINK](https://www.kaggle.com/code/awsaf49/prompt-recovery-with-gemma-kerasnlp-starter)*\n\n**Credits:** Awsaf (Owner); Ashley Chow (Editor); fchollet (Editor); Gusthema (Editor); Martin Görner (Editor); Paul Mooney (Editor); Phil Culliton (Editor).","metadata":{}},{"cell_type":"markdown","source":"# 1. Setup Modules & Dependencies","metadata":{}},{"cell_type":"code","source":"# Ignore Warnings\nfrom warnings import filterwarnings\nfilterwarnings('ignore')","metadata":{"execution":{"iopub.status.busy":"2024-04-13T04:22:51.767879Z","iopub.execute_input":"2024-04-13T04:22:51.768503Z","iopub.status.idle":"2024-04-13T04:22:51.779056Z","shell.execute_reply.started":"2024-04-13T04:22:51.768469Z","shell.execute_reply":"2024-04-13T04:22:51.778015Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Setup Environment [OS]\nimport os\nos.environ[\"KERAS_BACKEND\"] = \"jax\" # JAX backend for the best performance\nos.environ[\"XLA_PYTHON_CLIENT_MEM_FRACTION\"] = \"1.00\" # to avoid memory fragmentation on JAX backend\n\n# Natural Language Processing Modules and Machine Learning Modules\nimport keras\nimport keras_nlp\n\n# Numerical Data Processing Modules\nimport numpy as np\nimport pandas as pd\nfrom tqdm.notebook import tqdm\ntqdm.pandas() # progress bar for pandas\n\n# Data Visualization Modules\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nsns.set_theme(style='whitegrid', palette='viridis')\n\n# Markdown Display Modules\nfrom IPython.display import display, Markdown","metadata":{"execution":{"iopub.status.busy":"2024-04-13T04:22:54.755671Z","iopub.execute_input":"2024-04-13T04:22:54.756353Z","iopub.status.idle":"2024-04-13T04:23:09.276217Z","shell.execute_reply.started":"2024-04-13T04:22:54.756317Z","shell.execute_reply":"2024-04-13T04:23:09.275376Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 2. Configuration","metadata":{}},{"cell_type":"code","source":"# Setting up the configuration class\nclass CFG:\n    seed = 42\n    dataset_path = \"/kaggle/input/llm-prompt-recovery\"\n    preset = \"gemma_instruct_2b_en\" # name of pretrained Gemma\n    sequence_length = 850 # max size of input sequence for training\n    # We will using a different sequence_length for our dataset which will be the mean of the maximum sequence lengths of the final dataset\n    batch_size = 1 # size of the input batch in training\n    epochs=2 # for our training purpose as 10000 X 1 = 10000 seconds will be required for 1 epoch and hence for 2 epochs a training time of 20000s will be required which is close to 7 hours","metadata":{"execution":{"iopub.status.busy":"2024-04-13T04:26:56.966814Z","iopub.execute_input":"2024-04-13T04:26:56.967691Z","iopub.status.idle":"2024-04-13T04:26:56.972620Z","shell.execute_reply.started":"2024-04-13T04:26:56.967655Z","shell.execute_reply":"2024-04-13T04:26:56.971643Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 3. Reproducibility \nSets value for random seed to produce similar result in each run.","metadata":{}},{"cell_type":"code","source":"def random_setup(seed):\n    np.random.seed(seed)\n    keras.utils.set_random_seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    \nrandom_setup(CFG.seed)","metadata":{"execution":{"iopub.status.busy":"2024-04-13T04:26:59.853221Z","iopub.execute_input":"2024-04-13T04:26:59.854095Z","iopub.status.idle":"2024-04-13T04:26:59.859167Z","shell.execute_reply.started":"2024-04-13T04:26:59.854056Z","shell.execute_reply":"2024-04-13T04:26:59.858288Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 4. Data Upload\n\n**Data Format:**\n\nThese datasets includes:\n- `original_text`: Input text/essay that needs to be transformed.\n- `rewrite_prompt`: Prompt/Instruction that was used in the Gemma LM to transform `original_text`. This is also our **target** for this competition.\n- `rewritten_text`: Output text that was generated by the Gemma model.","metadata":{}},{"cell_type":"code","source":"df1=pd.read_csv(\"/kaggle/input/rewritten-texts-with-gemma-2b/rewritten_texts_csv.csv\",encoding = 'latin-1')\ndf1=df1[['original_text','prompt','rewritten_text']]\ndf1=df1.rename(columns={'prompt':'rewrite_prompt'})\ndf1=df1.head(11000)\n\ndf2=pd.read_csv(\"/kaggle/input/gemma-rewrite-nbroad/nbroad-v2.csv\")\ndf2=df2[['original_text','rewrite_prompt','rewritten_text']]\n\n# `LLM Prompt Recovery - Synthetic Datastore dataset` by @dschettler8845\ndf3 = pd.read_csv(\"/kaggle/input/llm-prompt-recovery-synthetic-datastore/gemma1000_w7b.csv\")\ndf3 = df3[[\"original_text\", \"rewrite_prompt\", \"gemma_7b_rewritten_text_temp0\"]]\ndf3 = df3.rename(columns={\"gemma_7b_rewritten_text_temp0\":\"rewritten_text\"})\n\n# `3000 Rewritten texts - Prompt recovery Challenge` by @dipamc77\ndf4 = pd.read_csv(\"/kaggle/input/3000-rewritten-texts-prompt-recovery-challenge/prompts_0_500_wiki_first_para_3000.csv\")\n\n# We will also use a kaggle dataset which is known as llm-prompt-recovery-data and available at https://www.kaggle.com/datasets/thedrcat/llm-prompt-recovery-data\ndf5=pd.read_csv(\"/kaggle/input/llm-prompt-recovery-data/gemma10000.csv\")\ndf5=df5[['original_text','rewrite_prompt','rewritten_text']]\ndf5=df5.head(7500) \n\ndf = pd.concat([df1, df2, df3, df4, df5], axis=0).dropna().reset_index(drop=True)\ndf=df.dropna().reset_index(drop=True)\ndf.drop_duplicates() # remove duplicate entries for training\ndf","metadata":{"execution":{"iopub.status.busy":"2024-04-13T04:23:17.826487Z","iopub.execute_input":"2024-04-13T04:23:17.827841Z","iopub.status.idle":"2024-04-13T04:23:20.355466Z","shell.execute_reply.started":"2024-04-13T04:23:17.827808Z","shell.execute_reply":"2024-04-13T04:23:20.354559Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Cleaning the Data","metadata":{}},{"cell_type":"code","source":"import re\ndef clean_text(text):\n    \n    text = text.replace(\"\\n\", \"\")\n    \n    text = re.sub(r'\\*\\*.*?\\*\\*', '', text)\n    return text\ndf['original_text']=df['original_text'].apply(clean_text)\ndf['rewritten_text'] = df['rewritten_text'].apply(clean_text)","metadata":{"execution":{"iopub.status.busy":"2024-04-13T04:23:27.653371Z","iopub.execute_input":"2024-04-13T04:23:27.654317Z","iopub.status.idle":"2024-04-13T04:23:27.799177Z","shell.execute_reply.started":"2024-04-13T04:23:27.654282Z","shell.execute_reply":"2024-04-13T04:23:27.798169Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def remove_symbols(text):\n    # Define regular expression pattern to match unnecessary symbols\n    pattern = r'[^\\w\\s]'\n    # Use re.sub() to replace matched symbols with an empty string\n    cleaned_text = re.sub(pattern, '', text)\n    return cleaned_text\ndf['original_text']= df['original_text'].apply(remove_symbols)\ndf['rewritten_text']= df['rewritten_text'].apply(remove_symbols)","metadata":{"execution":{"iopub.status.busy":"2024-04-13T04:23:30.120871Z","iopub.execute_input":"2024-04-13T04:23:30.121276Z","iopub.status.idle":"2024-04-13T04:23:32.413004Z","shell.execute_reply.started":"2024-04-13T04:23:30.121246Z","shell.execute_reply":"2024-04-13T04:23:32.412213Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df","metadata":{"execution":{"iopub.status.busy":"2024-04-13T04:23:35.039745Z","iopub.execute_input":"2024-04-13T04:23:35.040143Z","iopub.status.idle":"2024-04-13T04:23:35.051609Z","shell.execute_reply.started":"2024-04-13T04:23:35.040114Z","shell.execute_reply":"2024-04-13T04:23:35.050697Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# make a new column max_len containing the max length among all the columns of each row\ndf['max_len'] = df.apply(lambda row: max(len(row['original_text']), len(row['rewrite_prompt']), len(row['rewritten_text'])), axis=1)","metadata":{"execution":{"iopub.status.busy":"2024-04-13T04:23:38.577720Z","iopub.execute_input":"2024-04-13T04:23:38.578401Z","iopub.status.idle":"2024-04-13T04:23:39.064677Z","shell.execute_reply.started":"2024-04-13T04:23:38.578369Z","shell.execute_reply":"2024-04-13T04:23:39.063679Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Filter the new column by allowing all those lengths which are less than or equal to 2000\ndf = df[df['max_len'] <= 850]","metadata":{"execution":{"iopub.status.busy":"2024-04-13T04:23:42.235040Z","iopub.execute_input":"2024-04-13T04:23:42.235363Z","iopub.status.idle":"2024-04-13T04:23:42.243377Z","shell.execute_reply.started":"2024-04-13T04:23:42.235338Z","shell.execute_reply":"2024-04-13T04:23:42.242481Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Set background gradient style for description plot\ndf.describe().T.style.background_gradient(cmap='Blues')","metadata":{"execution":{"iopub.status.busy":"2024-04-13T04:23:46.468150Z","iopub.execute_input":"2024-04-13T04:23:46.468951Z","iopub.status.idle":"2024-04-13T04:23:46.566435Z","shell.execute_reply.started":"2024-04-13T04:23:46.468918Z","shell.execute_reply":"2024-04-13T04:23:46.565555Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plotting the max length\nplt.figure(figsize=(8, 6))\nsns.histplot(df['max_len'], bins=20, color='skyblue', edgecolor='black', kde=False)\nplt.title('Distribution of Maximum Lengths')\nplt.xlabel('Maximum Length')\nplt.ylabel('Frequency')\nplt.grid(True)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-04-13T04:23:56.861508Z","iopub.execute_input":"2024-04-13T04:23:56.862425Z","iopub.status.idle":"2024-04-13T04:23:57.308918Z","shell.execute_reply.started":"2024-04-13T04:23:56.862393Z","shell.execute_reply":"2024-04-13T04:23:57.308040Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['max_len'].mean()","metadata":{"execution":{"iopub.status.busy":"2024-04-13T04:24:04.549819Z","iopub.execute_input":"2024-04-13T04:24:04.550265Z","iopub.status.idle":"2024-04-13T04:24:04.556684Z","shell.execute_reply.started":"2024-04-13T04:24:04.550226Z","shell.execute_reply":"2024-04-13T04:24:04.555751Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"shuffled_df = df.sample(frac=1, random_state=42)\ndf=shuffled_df.reset_index(drop=True)\ndf","metadata":{"execution":{"iopub.status.busy":"2024-04-13T04:24:35.357867Z","iopub.execute_input":"2024-04-13T04:24:35.358250Z","iopub.status.idle":"2024-04-13T04:24:35.377884Z","shell.execute_reply.started":"2024-04-13T04:24:35.358222Z","shell.execute_reply":"2024-04-13T04:24:35.377027Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df=df.dropna().reset_index(drop=True)\ndf","metadata":{"execution":{"iopub.status.busy":"2024-04-13T04:24:53.119718Z","iopub.execute_input":"2024-04-13T04:24:53.120687Z","iopub.status.idle":"2024-04-13T04:24:53.145337Z","shell.execute_reply.started":"2024-04-13T04:24:53.120643Z","shell.execute_reply":"2024-04-13T04:24:53.144389Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 5. Prompt Engineering\n\nHere's a *custom prompt template* we'll use to create instruction-response pairs from the `original_text`, `rewritten_text`, and `rewrite_prompt`","metadata":{}},{"cell_type":"code","source":"# Advanced prompt engineering template\n\n# template = \"\"\"Instruction:\\nBelow, you'll find two texts: `Original Text` and `Rewritten Text`. The `Rewritten Text` has been generated from the `Original Text` using the LLM model Gemma 7b-it. Your task is to carefully analyze the similarities and differences between the two texts to determine the instruction or hint given to the LLM model to rewrite or transform the text.\\n\n# Please consider the following 8 questions while analyzing the texts:\\n\n# 1) What linguistic features have changed between the original and rewritten text, including sentence structure, vocabulary, tone, and style?\\t\n# 2) Are there any recurring patterns or repetitions in the rewritten text that suggest specific transformations applied by the model?\\t\n# 3) Does the rewritten text demonstrate a preference for particular word choices or syntactic structures?\\t\n# 4) How do the length and complexity of the rewritten text compare to the original text?\\t\n# 5) What contextual clues or hints in the original text may have influenced the transformation process?\\t\n# 6) How coherent and cohesive is the rewritten text compared to the original?\\t\n# 7) How readable and fluent is the rewritten text in effectively conveying the intended message?\\t\n# 8) Are there any implicit biases or sociocultural influences evident in the rewriting process?\\n\n# By carefully examining these aspects, try to deduce the underlying instruction or hint that guided the LLM model in rewriting the text from `Original Text` to `Rewritten Text`.\n# \\n\\nOriginal Text:\\n{original_text}\n# \\n\\nRewriten Text:\\n{rewritten_text}\n# \\n\\nResponse:\\n{rewrite_prompt}\"\"\"\n\ntemplate=\"\"\"Instruction:\\nYou are a skilled language model analyst with experience in text transformations. Help me determine the prompt an LLM might have used to rewrite an original text into a rewritten text.\\n\nWhen analyzing the two texts, please consider the following factors:\\n\n- Changes in tone and style.\\t\n- Shifts in language or vocabulary.\\t\n- Modifications to sentence structure or syntax.\\t\n- Additions, omissions, or alterations in content.\\t\n- Adjustments to the overall message or purpose of the text.\\n\nBased on your analysis, suggest the potential rewrite prompt that the LLM might have used to guide the transformation from the original text to the rewritten text.\n\\n\\nOriginal Text:\\n{original_text}\n\\n\\nRewriten Text:\\n{rewritten_text}\n\\n\\nResponse:\\n{rewrite_prompt}\"\"\"","metadata":{"execution":{"iopub.status.busy":"2024-04-13T04:25:06.032461Z","iopub.execute_input":"2024-04-13T04:25:06.033131Z","iopub.status.idle":"2024-04-13T04:25:06.039274Z","shell.execute_reply.started":"2024-04-13T04:25:06.033097Z","shell.execute_reply":"2024-04-13T04:25:06.038356Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Make a new column in the dataframe for storing the prompt template\ndf[\"prompt\"] = df.progress_apply(lambda row: template.format(original_text=row.original_text,\n                                                             rewritten_text=row.rewritten_text,\n                                                             rewrite_prompt=row.rewrite_prompt), axis=1)\n# Convert the prompt dataframe into a list for feeding into the model for training\ndata = df.prompt.tolist()","metadata":{"execution":{"iopub.status.busy":"2024-04-13T04:25:12.887048Z","iopub.execute_input":"2024-04-13T04:25:12.887633Z","iopub.status.idle":"2024-04-13T04:25:13.313777Z","shell.execute_reply.started":"2024-04-13T04:25:12.887600Z","shell.execute_reply":"2024-04-13T04:25:13.312896Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's examine a sample prompt. As the answers in our dataset are curated with **markdown** format, we will render the sample using `Markdown()` to properly visualize the formatting.","metadata":{}},{"cell_type":"code","source":"data[:10]","metadata":{"execution":{"iopub.status.busy":"2024-04-13T04:25:27.382531Z","iopub.execute_input":"2024-04-13T04:25:27.382895Z","iopub.status.idle":"2024-04-13T04:25:27.389825Z","shell.execute_reply.started":"2024-04-13T04:25:27.382863Z","shell.execute_reply":"2024-04-13T04:25:27.388951Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 5.1. Sample","metadata":{}},{"cell_type":"code","source":"# Text colorization function\ndef colorize_text(text):\n    for word, color in zip([\"Instruction\", \"Original Text\", \"Rewriten Text\", \"Response\"],\n                           [\"red\", \"purple\", \"blue\", \"green\"]):\n        text = text.replace(f\"{word}:\", f\"\\n\\n**<font color='{color}'>{word}:</font>**\")\n    return text","metadata":{"execution":{"iopub.status.busy":"2024-04-13T04:26:09.447946Z","iopub.execute_input":"2024-04-13T04:26:09.448803Z","iopub.status.idle":"2024-04-13T04:26:09.454116Z","shell.execute_reply.started":"2024-04-13T04:26:09.448768Z","shell.execute_reply":"2024-04-13T04:26:09.452938Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Take a random sample\nsample = data[10]\n\n# Give colors to Instruction, Response and Category\nsample = colorize_text(sample)\n\n# Show sample in markdown\ndisplay(Markdown(sample))","metadata":{"execution":{"iopub.status.busy":"2024-04-13T04:26:12.536662Z","iopub.execute_input":"2024-04-13T04:26:12.537232Z","iopub.status.idle":"2024-04-13T04:26:12.543519Z","shell.execute_reply.started":"2024-04-13T04:26:12.537199Z","shell.execute_reply":"2024-04-13T04:26:12.542590Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Initialising the model and getting a rough summary\ngemma_lm = keras_nlp.models.GemmaCausalLM.from_preset(CFG.preset)\ngemma_lm.summary()","metadata":{"execution":{"iopub.status.busy":"2024-04-13T04:27:10.445554Z","iopub.execute_input":"2024-04-13T04:27:10.446272Z","iopub.status.idle":"2024-04-13T04:28:13.396412Z","shell.execute_reply.started":"2024-04-13T04:27:10.446236Z","shell.execute_reply":"2024-04-13T04:28:13.395560Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 6. Inference before Fine-Tuning\n\nBefore we do fine-tuning, let's try to recover the prompt using the Gemma model with some prepared prompts and see how it responds.\n\n> As this model is not yet fine-tuned for instruction, you will notice that the model's responses are inaccurate.","metadata":{}},{"cell_type":"markdown","source":"## 6.1. Sample 1","metadata":{}},{"cell_type":"code","source":"# Take one sample\nrow = df.iloc[10]\n\n# Generate Prompt using template\nprompt = template.format(\n    original_text=row.original_text,\n    rewritten_text=row.rewritten_text,\n    rewrite_prompt=\"\",\n)\n\n# Infer\noutput = gemma_lm.generate(prompt, max_length=CFG.sequence_length)\n\n# Colorize\noutput = colorize_text(output)\n\n# Display in markdown\ndisplay(Markdown(output))\n","metadata":{"execution":{"iopub.status.busy":"2024-04-13T04:28:25.703564Z","iopub.execute_input":"2024-04-13T04:28:25.704209Z","iopub.status.idle":"2024-04-13T04:28:38.921960Z","shell.execute_reply.started":"2024-04-13T04:28:25.704178Z","shell.execute_reply":"2024-04-13T04:28:38.921042Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 6.2. Sample 2","metadata":{}},{"cell_type":"code","source":"# Take one sample\nrow = df.iloc[20]\n\n# Generate Prompt using template\nprompt = template.format(\n    original_text=row.original_text,\n    rewritten_text=row.rewritten_text,\n    rewrite_prompt=\"\",\n)\n\n# Infer\noutput = gemma_lm.generate(prompt, max_length=CFG.sequence_length)\n\n# Colorize\noutput = colorize_text(output)\n\n# Display in markdown\ndisplay(Markdown(output))\n","metadata":{"execution":{"iopub.status.busy":"2024-04-13T04:28:56.607308Z","iopub.execute_input":"2024-04-13T04:28:56.607652Z","iopub.status.idle":"2024-04-13T04:28:58.495567Z","shell.execute_reply.started":"2024-04-13T04:28:56.607626Z","shell.execute_reply":"2024-04-13T04:28:58.494566Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 7. Fine-tuning with LoRA","metadata":{}},{"cell_type":"code","source":"# Enable LoRA for the model and set the LoRA rank to 8.\ngemma_lm.backbone.enable_lora(rank=8) \ngemma_lm.summary()","metadata":{"execution":{"iopub.status.busy":"2024-04-13T04:29:26.112164Z","iopub.execute_input":"2024-04-13T04:29:26.113022Z","iopub.status.idle":"2024-04-13T04:29:26.541249Z","shell.execute_reply.started":"2024-04-13T04:29:26.112970Z","shell.execute_reply":"2024-04-13T04:29:26.540312Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Notice** that, the number of trainable parameters is reduced from ~$2.5$ billions to ~$2.7$ millions after enabling LoRA.","metadata":{}},{"cell_type":"markdown","source":"## 8. Training","metadata":{}},{"cell_type":"code","source":"# Limit the input sequence length (to control memory usage).\ngemma_lm.preprocessor.sequence_length = CFG.sequence_length \n\noptimizer = keras.optimizers.AdamW(\n    learning_rate=5e-5,\n    weight_decay=0.01,\n    )\n\noptimizer.exclude_from_weight_decay(var_names=[\"bias\", \"scale\"])\n\ngemma_lm.compile(\n    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n    optimizer=optimizer,\n    weighted_metrics=[keras.metrics.SparseCategoricalAccuracy()],\n)\n\n# Train model\ngemma_lm.fit(data, epochs=CFG.epochs, batch_size=CFG.batch_size, shuffle=True)","metadata":{"execution":{"iopub.status.busy":"2024-04-13T04:30:16.645453Z","iopub.execute_input":"2024-04-13T04:30:16.646186Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 9. Inference after fine-tuning\n\nLet's see how our fine-tuned model responds to the same questions we asked before fine-tuning the model.","metadata":{}},{"cell_type":"markdown","source":"## 9.1. Sample 1","metadata":{}},{"cell_type":"code","source":"# Take one sample\nrow = df.iloc[10]\n\n# Generate Prompt using template\nprompt = template.format(\n    original_text=row.original_text,\n    rewritten_text=row.rewritten_text,\n    rewrite_prompt=\"\",\n)\n\n# Infer\noutput = gemma_lm.generate(prompt, max_length=CFG.sequence_length)\n\n# Colorize\noutput = colorize_text(output)\n\n# Display in markdown\ndisplay(Markdown(output))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 9.2. Sample 2","metadata":{}},{"cell_type":"code","source":"# Take one sample\nrow = df.iloc[20]\n\n# Generate Prompt using template\nprompt = template.format(\n    original_text=row.original_text,\n    rewritten_text=row.rewritten_text,\n    rewrite_prompt=\"\",\n)\n\n# Infer\noutput = gemma_lm.generate(prompt, max_length=CFG.sequence_length)\n\n# Colorize\noutput = colorize_text(output)\n\n# Display in markdown\ndisplay(Markdown(output))\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 10. Test Data","metadata":{}},{"cell_type":"code","source":"# Reading the text data\ntest_df = pd.read_csv(\"/kaggle/input/llm-prompt-recovery/test.csv\")\ntest_df['original_text'] = test_df['original_text'].fillna(\"\")\ntest_df['rewritten_text'] = test_df['rewritten_text'].fillna(\"\")\ntest_df.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 10.1. Test Sample\n\nNow, let's try out a sample from test data that model hasn't seen during training.","metadata":{}},{"cell_type":"code","source":"# Loading a sample prompt from the test data\nrow = test_df.iloc[0]\n\n# Generate Prompt using template\nprompt = template.format(\n    original_text=row.original_text,\n    rewritten_text=row.rewritten_text,\n    rewrite_prompt=\"\",\n)\n\n# Infer\noutput = gemma_lm.generate(prompt, max_length=CFG.sequence_length)\n\n# Colorize\noutput = colorize_text(output)\n\n# Display in markdown\ndisplay(Markdown(output))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 11. Submission","metadata":{}},{"cell_type":"code","source":"# For storing the predictions\npreds = []\nfor i in tqdm(range(len(test_df))):\n    row = test_df.iloc[i]\n\n    # Generate Prompt using template\n    prompt = template.format(\n        original_text=row.original_text,\n        rewritten_text=row.rewritten_text,\n        rewrite_prompt=\"\"\n    )\n\n    # Infer\n    output = gemma_lm.generate(prompt, max_length=CFG.sequence_length)\n    pred = output.replace(prompt, \"\") # remove the prompt from output\n    \n    # Store predictions\n    preds.append([row.id, pred])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"While preparing the submission file, we must keep in mind that, leaving any `rewrite_prompt` blank as null answers will throw an error.","metadata":{}},{"cell_type":"code","source":"sub_df = pd.DataFrame(preds, columns=[\"id\", \"rewrite_prompt\"])\nsub_df['rewrite_prompt'] = sub_df['rewrite_prompt'].fillna(\"\")\nsub_df['rewrite_prompt'] = sub_df['rewrite_prompt'].map(lambda x: \"Make this text the best text ever!\" if len(x) == 0 else x)\nsub_df.to_csv(\"submission.csv\",index=False)\nsub_df.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 12. Save the Custom Model","metadata":{}},{"cell_type":"code","source":"gemma_lm.save(\"gemma_prompt_recovery_master.keras\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 13. Reference\n* [Fine-tune Gemma models in Keras using LoRA](https://www.kaggle.com/code/nilaychauhan/fine-tune-gemma-models-in-keras-using-lora)\n* [Parameter-efficient fine-tuning of GPT-2 with LoRA](https://keras.io/examples/nlp/parameter_efficient_finetuning_of_gpt2_with_lora/)\n* [Gemma - KerasNLP](https://keras.io/api/keras_nlp/models/gemma/)","metadata":{}}]}